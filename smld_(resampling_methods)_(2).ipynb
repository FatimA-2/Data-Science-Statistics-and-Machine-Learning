{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatimA-2/Data-Science-Statistics-and-Machine-Learning/blob/master/smld_(resampling_methods)_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dde3cef",
      "metadata": {
        "id": "6dde3cef"
      },
      "source": [
        "# Resampling Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9fd4324",
      "metadata": {
        "id": "a9fd4324"
      },
      "source": [
        "In this lab, we explore the resampling techniques covered in the part on Resampling Methods. We begin by placing most of our imports at this top level."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fiwcUtON4WlH",
      "metadata": {
        "id": "fiwcUtON4WlH"
      },
      "source": [
        "You will need to install the `ISLP` package, which provides access to the datasets and custom-built functions. This also installs most other packages needed in the labs. The Python resources page has a link to the ISLP documentation website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32mQ42r_zYDw",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "32mQ42r_zYDw",
        "outputId": "b0db64be-7bb3-42cc-931d-07094e2687ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ISLP\n",
            "  Downloading ISLP-0.4.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from ISLP) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.11/dist-packages (from ISLP) (1.14.1)\n",
            "Requirement already satisfied: pandas>=0.20 in /usr/local/lib/python3.11/dist-packages (from ISLP) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from ISLP) (5.3.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.11/dist-packages (from ISLP) (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from ISLP) (1.4.2)\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.11/dist-packages (from ISLP) (0.14.4)\n",
            "Collecting lifelines (from ISLP)\n",
            "  Downloading lifelines-0.30.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pygam (from ISLP)\n",
            "  Downloading pygam-0.9.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from ISLP) (2.6.0+cu124)\n",
            "Collecting pytorch-lightning (from ISLP)\n",
            "  Downloading pytorch_lightning-2.5.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting torchmetrics (from ISLP)\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.20->ISLP) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.20->ISLP) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.20->ISLP) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->ISLP) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13->ISLP) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13->ISLP) (24.2)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.11/dist-packages (from lifelines->ISLP) (3.10.0)\n",
            "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.11/dist-packages (from lifelines->ISLP) (1.7.0)\n",
            "Collecting autograd-gamma>=0.3 (from lifelines->ISLP)\n",
            "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting formulaic>=0.2.2 (from lifelines->ISLP)\n",
            "  Downloading formulaic-1.1.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: progressbar2<5.0.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pygam->ISLP) (4.5.0)\n",
            "Collecting scipy>=0.9 (from ISLP)\n",
            "  Downloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.7.1 (from ISLP)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning->ISLP) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning->ISLP) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning->ISLP) (4.13.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning->ISLP)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->ISLP) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->ISLP) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->ISLP) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->ISLP)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->ISLP)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->ISLP)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->ISLP)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->ISLP)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->ISLP)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->ISLP)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->ISLP)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->ISLP)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->ISLP) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->ISLP) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->ISLP) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->ISLP)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->ISLP) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->ISLP) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->ISLP) (1.3.0)\n",
            "Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines->ISLP)\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines->ISLP) (1.17.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning->ISLP) (75.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (3.2.3)\n",
            "Requirement already satisfied: python-utils>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from progressbar2<5.0.0,>=4.2.0->pygam->ISLP) (3.9.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.20->ISLP) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->ISLP) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (1.19.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (3.10)\n",
            "Downloading ISLP-0.4.0-py3-none-any.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lifelines-0.30.0-py3-none-any.whl (349 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.3/349.3 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygam-0.9.1-py3-none-any.whl (522 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m522.0/522.0 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.1-py3-none-any.whl (822 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading formulaic-1.1.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: autograd-gamma\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4030 sha256=1595d5a098dcc75e0bbac005fa648e7fff3db5b6efb31e3d0a324afbd772b9ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/67/f4/2caaae2146198dcb824f31a303833b07b14a5ec863fb3acd7b\n",
            "Successfully built autograd-gamma\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, lightning-utilities, interface-meta, scipy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pygam, nvidia-cusolver-cu12, formulaic, autograd-gamma, lifelines, torchmetrics, pytorch-lightning, ISLP\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n"
          ]
        }
      ],
      "source": [
        "%reset -f\n",
        "!pip install ISLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1deb5cc",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f1deb5cc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import (cross_validate, KFold, train_test_split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PeNrlSjW3dhS",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PeNrlSjW3dhS"
      },
      "outputs": [],
      "source": [
        "from ISLP import load_data\n",
        "from ISLP.models import (ModelSpec as MS, sklearn_sm, poly)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c04f8e4",
      "metadata": {
        "id": "1c04f8e4"
      },
      "source": [
        "## The Validation Set Approach\n",
        "We explore the use of the validation set approach in order to estimate the test error rates that result from fitting various linear models on the  `Auto`  data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f44ae0",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "22f44ae0"
      },
      "outputs": [],
      "source": [
        "Auto = load_data('Auto')\n",
        "Auto.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O3jYQlmbF8s5",
      "metadata": {
        "id": "O3jYQlmbF8s5"
      },
      "source": [
        "We use the function `train_test_split()` to split the data into training and validation sets. As there are 392 observations, we split into two equal sets of size 196 using the argument `test_size=196`. It is generally a good idea to set a random seed when performing operations like this that contain an element of randomness, so that the results obtained can be reproduced precisely at a later time. We set the random seed of the splitter with the argument `random_state=0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8kiPMzWIF-12",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8kiPMzWIF-12"
      },
      "outputs": [],
      "source": [
        "Auto_train, Auto_valid = train_test_split(Auto, test_size=196, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "318fe69f",
      "metadata": {
        "id": "318fe69f"
      },
      "source": [
        "Now we can fit a linear regression using only the observations corresponding to the training set `Auto_train`. We will use `ModelSpec()` (renamed `MS()` in the preamble) to create an object, which will then be used to construct the corresponding model matrix. For example, if we want to add \"horsepower\" as a predictor in our linear model, we will write:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c32e917",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0c32e917"
      },
      "outputs": [],
      "source": [
        "X = MS(['horsepower'])\n",
        "X_train = X.fit_transform(Auto_train)\n",
        "X_train.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F_gqcGRLByVC",
      "metadata": {
        "id": "F_gqcGRLByVC"
      },
      "source": [
        "`X_train` is the training model matrix, which includes a column of ones, necessary for estimating the intercept of the linear model, and a column associated with the predictor \"horsepower\", necessary for estimating the corresponding coefficient of the linear model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WLwhSh-PDVWW",
      "metadata": {
        "id": "WLwhSh-PDVWW"
      },
      "source": [
        "Let's prepare the training model vector for the response variable \"mpg\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "arTsrWZ3_p-V",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "arTsrWZ3_p-V"
      },
      "outputs": [],
      "source": [
        "y_train = Auto_train['mpg'] #mpg stands for \"miles per gallon\": it indicates how many miles we can travel on a gallon\n",
        "y_train.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qMUTsX40GyPd",
      "metadata": {
        "id": "qMUTsX40GyPd"
      },
      "source": [
        "Now we can estimate our linear model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Oke9Db0fEzTe",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Oke9Db0fEzTe"
      },
      "outputs": [],
      "source": [
        "model = sm.OLS(y_train, X_train)\n",
        "results = model.fit()\n",
        "print(results.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e883b8f",
      "metadata": {
        "id": "7e883b8f"
      },
      "source": [
        "We now use the `predict()` method of `results` evaluated on the training model matrix using the validation data set. First, we need to create the validation model matrix and the validation model vector for the response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86ce4f85",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "86ce4f85"
      },
      "outputs": [],
      "source": [
        "X_valid = X.transform(Auto_valid)\n",
        "y_valid = Auto_valid['mpg']\n",
        "y_valid.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MalJZWeDJ_qU",
      "metadata": {
        "id": "MalJZWeDJ_qU"
      },
      "source": [
        "Then, we can make predictions for the validation set using the results obtained from the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B2mXPjQNIQtT",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B2mXPjQNIQtT"
      },
      "outputs": [],
      "source": [
        "y_valid_pred = results.predict(X_valid)\n",
        "y_valid_pred.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OUBCLRjmIAD4",
      "metadata": {
        "id": "OUBCLRjmIAD4"
      },
      "source": [
        "The validation MSE of our model will be:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fjxo1f5NILGd",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fjxo1f5NILGd"
      },
      "outputs": [],
      "source": [
        "np.mean((y_valid - y_valid_pred)**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2ecdee6",
      "metadata": {
        "id": "f2ecdee6"
      },
      "source": [
        "Hence our estimate for the validation MSE of  the linear regression fit is $23.62$.\n",
        "\n",
        "We can also estimate the validation error for higher-degree polynomial regressions. We first provide a function `evalMSE()` that takes as input a string representing the model as well as a training and validation set and returns the MSE on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a66a97",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "50a66a97"
      },
      "outputs": [],
      "source": [
        "def evalMSE(predictors, response, train, valid):\n",
        "\n",
        "   X = MS(predictors)\n",
        "   X_train = X.fit_transform(train)\n",
        "   y_train = train[response]\n",
        "\n",
        "   X_valid = X.transform(valid)\n",
        "   y_valid = valid[response]\n",
        "\n",
        "   results = sm.OLS(y_train, X_train).fit()\n",
        "   y_valid_pred = results.predict(X_valid)\n",
        "\n",
        "   return np.mean((y_valid - y_valid_pred)**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a255779c",
      "metadata": {
        "id": "a255779c"
      },
      "source": [
        "Let’s use this function to estimate the validation MSE using linear, quadratic and cubic fits. We use the `enumerate()` function here, which gives both the values and indices of objects as one iterates over a for loop. Moreover, the `poly()` function supplied in `ISLP` specifies that columns representing polynomial functions of its first argument are added to the model matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d49b6999",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d49b6999"
      },
      "outputs": [],
      "source": [
        "MSE = np.zeros(3)\n",
        "for idx, degree in enumerate(range(1, 4)):\n",
        "    MSE[idx] = evalMSE([poly('horsepower', degree)], 'mpg', Auto_train, Auto_valid)\n",
        "MSE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d7b8fc1",
      "metadata": {
        "id": "9d7b8fc1"
      },
      "source": [
        "These error rates for the linear, quadratic and cubic fits are $23.62, 18.76$, and $18.80$, respectively.\n",
        "\n",
        "If we choose a different training/validation split instead, then we can expect somewhat different errors on the validation set. For example, let's consider a random seed for the splitter with argument `random_state=7` instead of `random_state=0`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dac8bd54",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dac8bd54"
      },
      "outputs": [],
      "source": [
        "Auto_train, Auto_valid = train_test_split(Auto, test_size=196, random_state=7)\n",
        "MSE = np.zeros(3)\n",
        "for idx, degree in enumerate(range(1, 4)):\n",
        "    MSE[idx] = evalMSE([poly('horsepower', degree)], 'mpg', Auto_train, Auto_valid)\n",
        "MSE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61f2c12d",
      "metadata": {
        "id": "61f2c12d"
      },
      "source": [
        "Using this split of the observations into a training set and a validation set, we find that the validation set error rates for the models with linear, quadratic, and cubic terms are $25.47$, $20.01$, and $19.96$, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dO7Mj5IZG8I2",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dO7Mj5IZG8I2"
      },
      "outputs": [],
      "source": [
        "%reset -f\n",
        "!pip install ISLP\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import (cross_validate, KFold, train_test_split)\n",
        "from ISLP import load_data\n",
        "from ISLP.models import (ModelSpec as MS, sklearn_sm, poly)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f22daa51",
      "metadata": {
        "id": "f22daa51"
      },
      "source": [
        "## Cross-Validation\n",
        "The simplest way to cross-validate in Python is to use `sklearn`, which has a different Application Programming Interface (API) than `statsmodels`, the library we have been using to fit generalized linear models (GLMs).\n",
        "\n",
        "This is a problem which often confronts data scientists: \"I have a function to do task $A$ (for example estimating GLMs with `statsmodels`), and need to feed it into something that performs task $B$ (for example cross-validating with `sklearn`), so that I can compute $B(A(D))$, where $D$ is my data.\" When $A$ and $B$ don’t naturally speak to each other, this requires the use of a *wrapper*.\n",
        "\n",
        "The `ISLP` package provides a wrapper, `sklearn_sm()`, that enables us to easily use the cross-validation tools of `sklearn` with models fit by `statsmodels`.\n",
        "\n",
        "The class `sklearn_sm()` has  as its first argument a model from `statsmodels`. It can take two additional optional arguments: `model_str` which can be used to specify a formula, and `model_args` which should be a dictionary of additional arguments used when fitting the model. For example, to fit a logistic regression model we have to specify a `family` argument. This\n",
        "is passed as `model_args={'family':sm.families.Binomial()}`.\n",
        "\n",
        "Here is our wrapper in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zTfCC7oIN1Kb",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zTfCC7oIN1Kb"
      },
      "outputs": [],
      "source": [
        "Auto = load_data('Auto')\n",
        "predictors = ['horsepower']\n",
        "model = sklearn_sm(sm.OLS, MS(predictors))\n",
        "X, Y = Auto[predictors], Auto['mpg']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WAjsqOhJRaQM",
      "metadata": {
        "id": "WAjsqOhJRaQM"
      },
      "source": [
        "Now, we will perform cross-validation using `cross_validate()`. The function takes the model to be estimated, the predictors, the response variable, and the argument `cv`, which specifies the details of the cross-validation folds. In particular, we used the `KFold()` function for the argument `cv`, which partitions the data into folds. Because we selected a number of folds equal to the total number of observations (i.e. `n_splits=len(Auto)`), we effectively performed leave-one-out cross-validation (LOOCV)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nMWePgIoKzYS",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nMWePgIoKzYS"
      },
      "outputs": [],
      "source": [
        "cv_folds = KFold(n_splits=len(Auto), shuffle=True, random_state=0) #LOOCV is not affected by the random_state!\n",
        "cv_results = cross_validate(model, X, Y, cv=cv_folds)\n",
        "print(\"Validation MSE for each fold:\")\n",
        "print(cv_results['test_score'])\n",
        "print(\"Number of folds: \" + str(len(cv_results['test_score'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebadc35f",
      "metadata": {
        "id": "ebadc35f"
      },
      "source": [
        "The LOOCV estimate for the test MSE is the average of these 392 test error estimates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nrhs7R1JFWQw",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nrhs7R1JFWQw"
      },
      "outputs": [],
      "source": [
        "cv_err = np.mean(cv_results['test_score'])\n",
        "cv_err #equal to 24.23"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25f47b99",
      "metadata": {
        "id": "25f47b99"
      },
      "source": [
        "We can repeat this procedure for increasingly complex polynomial fits. To automate the process, we again use a for loop which iteratively fits polynomial regressions of degree 1 to 3, computes the associated cross-validation error, and stores it in the $i$th element of the vector `cv_err`. The variable `d` in the for loop corresponds to the degree of the polynomial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11226c85",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "11226c85"
      },
      "outputs": [],
      "source": [
        "cv_err = np.zeros(3)\n",
        "cv_folds = KFold(n_splits=len(Auto), shuffle=True, random_state=0) #LOOCV is not affected by the random_state!\n",
        "predictors = np.array(Auto['horsepower'])\n",
        "model = sklearn_sm(sm.OLS)\n",
        "\n",
        "for i, d in enumerate(range(1,4)):\n",
        "    X = np.power.outer(predictors, np.arange(d+1))\n",
        "    cv_results = cross_validate(model, X, Y, cv=cv_folds)\n",
        "    cv_err[i] = np.mean(cv_results['test_score'])\n",
        "cv_err"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3a920ae",
      "metadata": {
        "id": "a3a920ae"
      },
      "source": [
        "Above we introduced `np.power.outer(a, b)`. It is a NumPy function that computes the outer product of powers. In other words, it raises each element of `a` to each element of `b`.\n",
        "\n",
        "`np.arange(d+1)` creates an array of integers from 0 to `d`. For example, if `d` = 3, the resulting array will be [0, 1, 2, 3]. So, the outer product of powers will be: $predictor^0 = 1$, $predictor^1 = predictor$, $predictor^2$, $predictor^3$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71385c1b",
      "metadata": {
        "id": "71385c1b"
      },
      "source": [
        "In the CV above (LOOCV), we set the number of folds equal to $n$, where $n$ is the number of observations. Of course we can also use a number of folds less than $n$. The code is very similar to the above (and is significantly faster). Here we use `KFold()` to partition the data into $10$ random folds. We use `random_state` to set a random seed and initialize a vector `cv_err` in which we will store the CV errors corresponding to the polynomial fits of degrees one to three."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca0f972f",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ca0f972f"
      },
      "outputs": [],
      "source": [
        "cv_err = np.zeros(3)\n",
        "cv_folds = KFold(n_splits=10, shuffle=True, random_state=0) #k-Fold is affected by the random_state when k < n!\n",
        "predictors = np.array(Auto['horsepower'])\n",
        "model = sklearn_sm(sm.OLS)\n",
        "\n",
        "for i, d in enumerate(range(1,4)):\n",
        "    X = np.power.outer(predictors, np.arange(d+1))\n",
        "    cv_results = cross_validate(model, X, Y, cv=cv_folds)\n",
        "    cv_err[i] = np.mean(cv_results['test_score'])\n",
        "cv_err"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vGCm9yJ2o5Fh",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vGCm9yJ2o5Fh"
      },
      "outputs": [],
      "source": [
        "%reset -f\n",
        "!pip install ISLP\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import (cross_validate, KFold, train_test_split)\n",
        "from ISLP import load_data\n",
        "from ISLP.models import (ModelSpec as MS, sklearn_sm, poly)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07165f0e",
      "metadata": {
        "id": "07165f0e"
      },
      "source": [
        "## The Bootstrap\n",
        "We illustrate the use of the bootstrap in the simple example where we supposed that we wish to invest a fixed sum of money in two financial\n",
        "assets that yield returns of $X$ and $Y$. The goal is to estimate the\n",
        "sampling variance of the parameter $\\alpha$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PhjsNGpeoM6-",
      "metadata": {
        "id": "PhjsNGpeoM6-"
      },
      "source": [
        "After loading the `Portfolio` dataset from the `ISLP` package, we will\n",
        "create a function `alpha_func()`, which takes as input a dataframe `D` assumed to have columns `X` and `Y`, as well as a vector `idx` indicating which observations should be used to estimate\n",
        "$\\alpha$. The function then outputs the estimate for $\\alpha$ based on\n",
        "the selected observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4b6d9b3",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a4b6d9b3"
      },
      "outputs": [],
      "source": [
        "Portfolio = load_data('Portfolio')\n",
        "def alpha_func(D, idx):\n",
        "   cov_ = np.cov(D[['X','Y']].loc[idx], rowvar=False)\n",
        "   return ((cov_[1,1]-cov_[0,1]) / (cov_[0,0]+cov_[1,1]-2*cov_[0,1]))\n",
        "Portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rIJy_5DCJjgq",
      "metadata": {
        "id": "rIJy_5DCJjgq"
      },
      "source": [
        "Since the observations in the dataset are indexed with an index ranging from 0 to 99, if we want to estimate $\\alpha$ considering all the observations in the dataset, we can use `np.arange(100)` in the `idx` argument. Indeed, `np.arange(100)` will return an array of values ranging from 0 to 99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w1dbWL1RGJYm",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w1dbWL1RGJYm"
      },
      "outputs": [],
      "source": [
        "np.arange(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d50058e",
      "metadata": {
        "id": "9d50058e"
      },
      "source": [
        "Let's estimate $\\alpha$ using all 100 observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81498a11",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "81498a11"
      },
      "outputs": [],
      "source": [
        "alpha_func(Portfolio, np.arange(100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oxGXX1zILIl5",
      "metadata": {
        "id": "oxGXX1zILIl5"
      },
      "source": [
        "From the `Portfolio` dataset, we can draw bootstrap datasets, whose characteristic is that observations are sampled from the original dataset with replacement. We can perform sampling with replacement using a random number generator (RNG) with `np.random.default_rng(0)`. In particular"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AsONQMEXGHoD",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AsONQMEXGHoD"
      },
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(0) #random number generator with seed 0. This makes the random number generator reproducible\n",
        "rng.choice(np.arange(100), 100, replace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91wIhO7nNXGP",
      "metadata": {
        "id": "91wIhO7nNXGP"
      },
      "source": [
        "`rng.choice(np.arange(100), 100, replace=True)` returned an array of 100 pseudorandom numbers (as specified by the second argument, `100`), selected from the range 0 to 99 (as defined by the first argument, `np.arange(100)`), with replacement (`replace=True`). These numbers represent the indices of the observations from the original dataset that will be included in the bootstrap dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91a635fe",
      "metadata": {
        "id": "91a635fe"
      },
      "source": [
        "Let's estimate the bootstrap standard error of $\\hat{\\alpha}$. We can do this by creating a simple function, `boot_SE()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd16bbae",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dd16bbae"
      },
      "outputs": [],
      "source": [
        "def boot_SE(func, D, n=None, B=1000, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    first, second = 0, 0 #values used to calculate the SE bootstrap\n",
        "    n = n or len(D) #if the third argument of the function is None, n = len(D); if the argument is n, n = n\n",
        "    for _ in range(B):\n",
        "        idx = rng.choice(D.index, n, replace=True)\n",
        "        value = func(D, idx)\n",
        "        first += value\n",
        "        second += value**2\n",
        "    return np.sqrt(second / B - (first / B)**2) #square_root(E(alpha^2) - (E(alpha))^2)\n",
        "\n",
        "    #Note that E(alpha^2) - (E(alpha))^2 is an alternative formula for the variance, i.e.:\n",
        "    #Var(alpha) = E(alpha - E(alpha))^2 = E(alpha^2) - (E(alpha))^2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac4e17ed",
      "metadata": {
        "id": "ac4e17ed"
      },
      "source": [
        "Notice the use of `_` as a loop variable in `for _ in range(B)`. This is often used if the value of the counter is unimportant and simply makes sure  the loop is executed `B` times.\n",
        "\n",
        "Let’s use our function to evaluate the accuracy of our estimate of $\\alpha$ using $B=1000$ bootstrap replications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b42b4585",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b42b4585"
      },
      "outputs": [],
      "source": [
        "alpha_SE = boot_SE(func=alpha_func, D=Portfolio, B=1000, seed=0)\n",
        "alpha_SE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "su-53y-G0yC0",
      "metadata": {
        "id": "su-53y-G0yC0"
      },
      "source": [
        "The final output shows that the bootstrap estimate for ${\\rm SE}(\\hat{\\alpha})$ is $0.0912$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pIqptqAC7kUX",
      "metadata": {
        "id": "pIqptqAC7kUX"
      },
      "source": [
        "##References\n",
        "This Jupyter Notebook is a revised version curated by Paolo Mustica, based on the original Notebook provided by the authors James, G., Witten, D., Hastie, T., Tibshirani, R., and Taylor, J. (2023) in An Introduction to Statistical Learning with Applications in Python (Chapter 5), published by Springer Nature."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}